#  Linear Regression
###### Formula:
$$ h(x) = \theta_0 + \theta_1 x $$
<div style="text-align: center;"><i>or</i></div>

$$
\hat{y} = \beta_0 + \beta_1 x
$$
- $h(x)$, $\hat{y}$ : Dependent variable / Predicted variables
- $x$ : Independent variable
- $\theta_1$, $\beta_1$ : Slope or coefficients
- $\theta_0$, $\beta_0$ : Intercept
---
###### Cost function (Mean Squared Error)
$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \big(h_\theta(x^{(i)}) - y^{(i)}\big)^2
$$
- $h_\theta(x^{(i)})$ : Dependent variable / Predicted variables
- $y^{(i)}$ : Original output value
---

###### Graph:
![Linear Regression Example](https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg)
